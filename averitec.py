# -*- coding: utf-8 -*-
"""averitec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kKqgxSYoqsR9JXZ_Qq5a6gtVoaVSMhnZ
"""

pip install pandas scikit-learn nltk rank_bm25 jsonlines

import jsonlines
import pandas as pd
import nltk
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report

# Load JSONL Data
def load_data(file_path):
    data = []
    try:
        with jsonlines.open(file_path) as reader:
            for obj in reader:
                data.append(obj)
    except jsonlines.InvalidLineError as e:
        print(f"Error parsing JSONL file: {e}")
        # Print the problematic line for debugging:
        print(f"Problematic line: {e.line}")
        # If you know the error, you can add logic to fix it here.
        # For example, if it's a missing comma, you could try to insert it.
        return pd.DataFrame([])  # Return empty DataFrame in case of error
    return pd.DataFrame(data)

# Load Train & Test Data
train_df = load_data("train.json")
test_df = load_data("test.json")

# Check data structure
print(train_df.head())

pip install pandas

import pandas as pd

# Load JSON file into DataFrame
df = pd.read_json("train.json")

# Display DataFrame
print(df.head())

import pandas as pd

# Load JSON file into DataFrame
df = pd.read_json("test.json")

# Display DataFrame
print(df.head())

import nltk
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Download the missing punkt_tab data

STOPWORDS = set(stopwords.words('english'))

# Text Cleaning Function
def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'\W+', ' ', text)  # Remove special characters
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    tokens = word_tokenize(text)  # Tokenization
    tokens = [word for word in tokens if word not in STOPWORDS]  # Remove stopwords
    return " ".join(tokens)

# Apply Preprocessing
df["clean_claim"] = df["claim"].apply(preprocess_text)
print(df.head())

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000)

# Transform the claim text into numerical vectors
X = vectorizer.fit_transform(df["clean_claim"])

# Convert labels into numerical values
label_map = {"Supported": 0, "Refuted": 1, "Not Enough Evidence": 2, "Conflicting Evidence/Cherry-picking": 3}
df["label_encoded"] = df["label"].map(label_map)

# Display shape of the dataset
print(f"Feature Matrix Shape: {X.shape}")

print(df.columns)

import json

# Load the JSON file and print an example entry
with open("train.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Print the first entry
print(data[0].keys())  # This will show all available keys in the dataset

import pandas as pd
import json

# Load JSON file
with open("train.json", "r", encoding="utf-8") as f:
    data = json.load(f)  # Load full JSON list

# Convert JSON list to DataFrame
df = pd.DataFrame(data)

# Check if "label" is now included
print(df.columns)

import nltk
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Download the missing punkt_tab data

STOPWORDS = set(stopwords.words('english'))

# Text Cleaning Function
def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'\W+', ' ', text)  # Remove special characters
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    tokens = word_tokenize(text)  # Tokenization
    tokens = [word for word in tokens if word not in STOPWORDS]  # Remove stopwords
    return " ".join(tokens)

# Apply Preprocessing
df["clean_claim"] = df["claim"].apply(preprocess_text)


# Convert labels into numerical values, handling unknown labels
label_map = {"Supported": 0, "Refuted": 1, "Not Enough Evidence": 2, "Conflicting Evidence/Cherry-picking": 3}
# If a label is not found in label_map, it will be assigned -1 or any other suitable value
df["label_encoded"] = df["label"].map(label_map).fillna(-1).astype(int)
#fillna replaces NaN with -1. astype(int) converts to integers.

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000)

# Transform the claim text into numerical vectors
X = vectorizer.fit_transform(df["clean_claim"])

# Convert labels into numerical values
label_map = {"Supported": 0, "Refuted": 1, "Not Enough Evidence": 2, "Conflicting Evidence/Cherry-picking": 3}
df["label_encoded"] = df["label"].map(label_map)

# Display shape of the dataset
print(f"Feature Matrix Shape: {X.shape}")

print(df["label"].isna().sum())  # Count NaN values in label column

df = df.dropna(subset=["label"])

df["label"].fillna("Not Enough Evidence", inplace=True)

print("NaN values in label column:", df["label"].isna().sum())  # Check NaNs in the original label column
print("NaN values in label_encoded column:", df["label_encoded"].isna().sum())  # Check NaNs in the encoded label column

# Find unique labels in the dataset
print("Unique labels in dataset:", df["label"].unique())

# Find labels that were not mapped correctly
unmapped_labels = df[df["label_encoded"].isna()]["label"].unique()
print("Labels that caused NaN in encoding:", unmapped_labels)

label_map = {
    "Supported": 0,
    "Refuted": 1,
    "Not Enough Evidence": 2,
    "Conflicting Evidence/Cherry-picking": 3
}

# If there are variations in spelling, add them to the map
df["label_encoded"] = df["label"].map(label_map)

# Fill remaining NaN labels with "Not Enough Evidence" (default class)
df["label_encoded"].fillna(2, inplace=True)  # Assign 2 = "Not Enough Evidence"

# Convert to integer
df["label_encoded"] = df["label_encoded"].astype(int)

print("NaN values in label_encoded after fixing:", df["label_encoded"].isna().sum())  # Should be 0

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, df["label_encoded"], test_size=0.2, random_state=42)

# Train Model
clf = LogisticRegression(max_iter=500)
clf.fit(X_train, y_train)

# Make Predictions
y_pred = clf.predict(X_test)

# Model Evaluation
print("Model Evaluation:")
print(classification_report(y_test, y_pred))

from rank_bm25 import BM25Okapi

# Tokenize evidence text
df["tokenized_evidence"] = df["evidence"].apply(lambda x: nltk.word_tokenize(str(x).lower()))

# Initialize BM25 Model
bm25 = BM25Okapi(df["tokenized_evidence"])

# Function to Retrieve Best Evidence for a Claim
def retrieve_best_evidence(claim):
    tokenized_claim = nltk.word_tokenize(claim.lower())
    scores = bm25.get_scores(tokenized_claim)
    best_evidence_idx = scores.argmax()
    return df.iloc[best_evidence_idx]["evidence"]fg,ktdiu8rex7w4z

# Apply evidence retrieval to test set
df["retrieved_evidence"] = df["clean_claim"].apply(retrieve_best_evidence)

print("Evidence Retrieval Completed!")

"""ev2r score

"""

# Predict labels using trained Logistic Regression model
y_pred = clf.predict(X_test)

# Convert numerical predictions back to label names
reverse_label_map = {0: "Supported", 1: "Refuted", 2: "Not Enough Evidence", 3: "Conflicting Evidence/Cherry-picking"}
df_test["predicted_label"] = [reverse_label_map[p] for p in y_pred]

# Check structure
print(df_test[["claim", "predicted_label"]].head())

X_train, X_test, y_train, y_test = train_test_split(X, df["label_encoded"], test_size=0.2, random_state=42)

df_test = df.iloc[y_test.index]  # Use y_test.index instead of X_test.index

# Convert numerical predictions back to text labels
reverse_label_map = {0: "Supported", 1: "Refuted", 2: "Not Enough Evidence", 3: "Conflicting Evidence/Cherry-picking"}

# Assign predicted labels
df_test["predicted_label"] = [reverse_label_map[p] for p in y_pred]

# Check structure
print(df_test[["claim", "predicted_label"]].head())

# Verify that retrieved and reference evidence exist
print(df_test[["retrieved_evidence", "reference_evidence"]].head())