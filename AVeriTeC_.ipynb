{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrBE9Oiul4mo"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas scikit-learn nltk rank_bm25 jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFbhLNGPl-xM",
        "outputId": "850494c0-21a8-4e8c-d8f1-830fe39a95d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP 1: Load JSON or JSONL Data Correctly ----"
      ],
      "metadata": {
        "id": "FPuh0XJycULC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            first_char = f.read(1)\n",
        "            if first_char == \"[\":\n",
        "                f.seek(0)  # Reset file pointer\n",
        "                data = pd.read_json(f)  # Load JSON array\n",
        "            else:\n",
        "                f.seek(0)\n",
        "                data = []\n",
        "                with jsonlines.open(file_path) as reader:\n",
        "                    for obj in reader:\n",
        "                        data.append(obj)\n",
        "                data = pd.DataFrame(data)  # Convert to DataFrame\n",
        "        if data.empty:\n",
        "            print(f\"⚠️ Warning: {file_path} is empty or improperly formatted!\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"🚨 Error loading {file_path}: {e}\")\n",
        "        return pd.DataFrame([])  # Return empty DataFrame on failure\n"
      ],
      "metadata": {
        "id": "re1jIyGhmB92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = load_data(\"train.json\")\n",
        "test_df = load_data(\"dev.json\")"
      ],
      "metadata": {
        "id": "J10e-8grmJ32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load train.json\n",
        "with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# Convert to DataFrame\n",
        "train_df = pd.DataFrame(train_data)\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = train_df[\"label\"].value_counts()\n",
        "\n",
        "# Display results\n",
        "print(\"📊 Label Distribution in train.json:\")\n",
        "print(label_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbFcusPXkvWB",
        "outputId": "7de82e8f-b10e-4635-c23b-340bef0c8120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Label Distribution in train.json:\n",
            "label\n",
            "Refuted                               1742\n",
            "Supported                              849\n",
            "Not Enough Evidence                    282\n",
            "Conflicting Evidence/Cherrypicking     195\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load train.json\n",
        "with open(\"dev.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# Convert to DataFrame\n",
        "train_df = pd.DataFrame(train_data)\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = train_df[\"label\"].value_counts()\n",
        "\n",
        "# Display results\n",
        "print(\"📊 Label Distribution in dev.json:\")\n",
        "print(label_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LKn0Lzo0kGW",
        "outputId": "ba8a3540-99f4-4cd4-93e3-809843972144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Label Distribution in dev.json:\n",
            "label\n",
            "Refuted                               305\n",
            "Supported                             122\n",
            "Conflicting Evidence/Cherrypicking     38\n",
            "Not Enough Evidence                    35\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_claims = len(train_df)\n",
        "print(f\"📌 Total number of claims in train.json: {total_claims}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ-Wn2jyk2yl",
        "outputId": "b89cf3e9-94ba-4b7b-b128-d275a1cb3349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Total number of claims in train.json: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_claims = len(test_df)\n",
        "print(f\"📌 Total number of claims in test.json: {total_claims}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbBkmdLl1jQN",
        "outputId": "41a86458-0837-4976-a5bc-293fa431aa50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Total number of claims in test.json: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: Extract Evidence from Available Fields"
      ],
      "metadata": {
        "id": "-UhyuORPcZzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_evidence(row):\n",
        "    if isinstance(row.get(\"fact_checking_article\"), str) and len(row[\"fact_checking_article\"]) > 10:\n",
        "        return row[\"fact_checking_article\"]\n",
        "    elif isinstance(row.get(\"questions\"), list) and len(row[\"questions\"]) > 0:\n",
        "        return \" \".join([q.get(\"question\", \"\") for q in row[\"questions\"]])\n",
        "    elif isinstance(row.get(\"justification\"), str) and len(row[\"justification\"]) > 10:\n",
        "        return row[\"justification\"]\n",
        "    return \"No evidence available\"\n"
      ],
      "metadata": {
        "id": "9AjiUGFWmMy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply evidence extraction\n",
        "train_df[\"evidence\"] = train_df.apply(extract_evidence, axis=1)\n",
        "test_df[\"evidence\"] = test_df.apply(extract_evidence, axis=1)\n"
      ],
      "metadata": {
        "id": "ElSQQsEsmQZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2owR0pd0my8h",
        "outputId": "be86c6b4-e68c-4eae-d62f-9fa09f233c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize evidence\n",
        "train_df[\"tokenized_evidence\"] = train_df[\"evidence\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n",
        "test_df[\"tokenized_evidence\"] = test_df[\"evidence\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n"
      ],
      "metadata": {
        "id": "dH9V6Ar1mTAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: BM25 Evidence Retrieval"
      ],
      "metadata": {
        "id": "0N5PdPj-cdiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bm25 = BM25Okapi(train_df[\"tokenized_evidence\"])\n",
        "\n",
        "def retrieve_best_evidence(claim):\n",
        "    tokenized_claim = nltk.word_tokenize(claim.lower())\n",
        "    scores = bm25.get_scores(tokenized_claim)\n",
        "    best_evidence_idx = scores.argmax()\n",
        "    return train_df.iloc[best_evidence_idx][\"evidence\"] if scores.max() > 0 else \"No evidence found\"\n"
      ],
      "metadata": {
        "id": "gJ0bIENhm1B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve evidence for test set\n",
        "test_df[\"retrieved_evidence\"] = test_df[\"claim\"].apply(retrieve_best_evidence)\n"
      ],
      "metadata": {
        "id": "GVibEnUEm4AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if all keys from test.json are converted to columns in test_df\n",
        "import json\n",
        "# Load test.json to inspect its structure\n",
        "test_json_path = \"/content/dev.json\"\n",
        "\n",
        "try:\n",
        "    with open(test_json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        test_data = json.load(f)  # Load test.json as a list of dictionaries\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    test_df_fixed = pd.DataFrame(test_data)\n",
        "\n",
        "    # Compare columns from JSON with test_df\n",
        "    json_keys = set(test_data[0].keys()) if len(test_data) > 0 else set()\n",
        "    df_columns = set(test_df_fixed.columns)\n",
        "\n",
        "    missing_keys = json_keys - df_columns\n",
        "\n",
        "    # Display results\n",
        "    print(f\"🛠 Columns in test_df: {df_columns}\")\n",
        "    print(f\"🔍 Keys in test.json: {json_keys}\")\n",
        "    print(f\"🚨 Missing keys in test_df: {missing_keys}\" if missing_keys else \"✅ All keys from test.json are in test_df!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"🚨 Error loading test.json: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGZjmTztstM2",
        "outputId": "60c3e9da-7c7f-4ca8-a8f5-423996b90cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛠 Columns in test_df: {'original_claim_url', 'justification', 'claim', 'claim_date', 'label', 'reporting_source', 'claim_types', 'questions', 'fact_checking_strategies', 'cached_original_claim_url', 'required_reannotation', 'fact_checking_article', 'speaker', 'location_ISO_code'}\n",
            "🔍 Keys in test.json: {'original_claim_url', 'justification', 'claim', 'claim_date', 'label', 'reporting_source', 'claim_types', 'questions', 'fact_checking_strategies', 'cached_original_claim_url', 'required_reannotation', 'fact_checking_article', 'speaker', 'location_ISO_code'}\n",
            "✅ All keys from test.json are in test_df!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4: Encode Labels for Model Training"
      ],
      "metadata": {
        "id": "MO1HKpoJci6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "label_map = {\"Supported\": 0, \"Refuted\": 1, \"Not Enough Evidence\": 2, \"Conflicting Evidence/Cherrypicking\": 3}\n",
        "train_df[\"label_encoded\"] = train_df[\"label\"].map(label_map)\n",
        "test_df[\"label_encoded\"] = test_df[\"label\"].map(label_map)\n",
        "\n",
        "# Remove NaN values from labels\n",
        "train_df = train_df.dropna(subset=[\"label_encoded\"])\n",
        "test_df = test_df.dropna(subset=[\"label_encoded\"])"
      ],
      "metadata": {
        "id": "8zwlNXtsnBe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T76V3f3D1wo8",
        "outputId": "43f37aeb-1a57-452b-8afc-582355be178f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['claim', 'required_reannotation', 'label', 'justification',\n",
            "       'claim_date', 'speaker', 'original_claim_url', 'fact_checking_article',\n",
            "       'reporting_source', 'location_ISO_code', 'claim_types',\n",
            "       'fact_checking_strategies', 'questions', 'cached_original_claim_url',\n",
            "       'evidence', 'tokenized_evidence', 'retrieved_evidence',\n",
            "       'label_encoded'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)  # List all column names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66y8LZWul2fp",
        "outputId": "1e8052f2-47dd-4bb3-cea1-56cf91e3e30e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['claim', 'required_reannotation', 'label', 'justification',\n",
            "       'claim_date', 'speaker', 'original_claim_url', 'fact_checking_article',\n",
            "       'reporting_source', 'location_ISO_code', 'claim_types',\n",
            "       'fact_checking_strategies', 'questions', 'cached_original_claim_url',\n",
            "       'evidence', 'tokenized_evidence', 'label_encoded'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in train_df:\", test_df.columns)\n",
        "print(test_df[[\"label\", \"label_encoded\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM7d4y9c16t8",
        "outputId": "4749886d-3c82-4aa1-dc57-be8fca89591c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in train_df: Index(['claim', 'required_reannotation', 'label', 'justification',\n",
            "       'claim_date', 'speaker', 'original_claim_url', 'fact_checking_article',\n",
            "       'reporting_source', 'location_ISO_code', 'claim_types',\n",
            "       'fact_checking_strategies', 'questions', 'cached_original_claim_url',\n",
            "       'evidence', 'tokenized_evidence', 'retrieved_evidence',\n",
            "       'label_encoded'],\n",
            "      dtype='object')\n",
            "     label  label_encoded\n",
            "0  Refuted              1\n",
            "1  Refuted              1\n",
            "2  Refuted              1\n",
            "3  Refuted              1\n",
            "4  Refuted              1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df[\"label\"].unique())  # Check unique text labels\n",
        "print(test_df[\"label_encoded\"].unique())  # Check unique encoded labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx0GMy30lQRC",
        "outputId": "27577ed6-d383-4c49-d419-db36618e09c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Refuted' 'Supported' 'Not Enough Evidence'\n",
            " 'Conflicting Evidence/Cherrypicking']\n",
            "[1 0 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df[\"label\"].value_counts())  # Show all unique label texts in test set\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjiHK0VMmeHy",
        "outputId": "7538bdd8-147a-4ff4-acf1-9930d5981f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "Refuted                               305\n",
            "Supported                             122\n",
            "Conflicting Evidence/Cherrypicking     38\n",
            "Not Enough Evidence                    35\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"🚨 Missing labels in test_df: {test_df['label'].isna().sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amovJgLZ6rDE",
        "outputId": "61650ae7-94d0-4b8b-cd79-30886de147b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚨 Missing labels in test_df: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df.dropna(subset=[\"label\"])\n"
      ],
      "metadata": {
        "id": "LMU6D3hO69tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load test.json again\n",
        "with open(\"dev.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "test_df = pd.DataFrame(test_data)  # Ensure full dataset is loaded\n",
        "print(\"✅ Reloaded test data!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5siMFu5q6yhM",
        "outputId": "7c5dcff8-42e7-4178-afc1-0ed989f5eecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Reloaded test data!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    \"Supported\": 0,\n",
        "    \"Refuted\": 1,\n",
        "    \"Not Enough Evidence\": 2,\n",
        "    \"Conflicting Evidence/Cherrypicking\": 3  # Ensure exact match\n",
        "}\n",
        "\n",
        "test_df.loc[:, \"label_encoded\"] = test_df[\"label\"].map(label_map)\n",
        "print(\"✅ Applied label encoding!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOdz1mKY6z-J",
        "outputId": "3856170c-9df5-45b1-de57-543f62ddc9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Applied label encoding!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📊 Unique Labels in test_df['label']:\", test_df[\"label\"].unique())\n",
        "print(\"📊 Unique Encoded Labels:\", test_df[\"label_encoded\"].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkgGmXhD63A8",
        "outputId": "9dc97509-39c7-49c4-a5a3-b06170ab1747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Unique Labels in test_df['label']: ['Refuted' 'Supported' 'Not Enough Evidence'\n",
            " 'Conflicting Evidence/Cherrypicking']\n",
            "📊 Unique Encoded Labels: [1 0 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load train.json again\n",
        "with open(\"dev.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "train_df = pd.DataFrame(train_data)  # Ensure all data is loaded\n",
        "print(f\"✅ Loaded dev.json with {len(test_df)} samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INxR-J3QnFuZ",
        "outputId": "ae5e432e-5f5e-4b7c-f586-33b6e7e263ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded dev.json with 500 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📊 Unique Labels in train_df['label']:\", train_df[\"label\"].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqX0fDvFnJBl",
        "outputId": "456a1247-6910-4029-fa6d-d4efac8537ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Unique Labels in train_df['label']: ['Refuted' 'Supported' 'Not Enough Evidence'\n",
            " 'Conflicting Evidence/Cherrypicking']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📊 Unique Labels in train_df['label']:\", test_df[\"label\"].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b8rBGZwpce1",
        "outputId": "c4d1f3b1-9a06-4ccc-b6ab-c38644044217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Unique Labels in train_df['label']: ['Refuted' 'Supported' 'Not Enough Evidence'\n",
            " 'Conflicting Evidence/Cherrypicking']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define correct label mapping\n",
        "label_map = {\n",
        "    \"Supported\": 0,\n",
        "    \"Refuted\": 1,\n",
        "    \"Not Enough Evidence\": 2,\n",
        "    \"Conflicting Evidence/Cherrypicking\": 3  # Ensure correct mapping\n",
        "}\n",
        "\n",
        "# Apply encoding safely\n",
        "train_df.loc[:, \"label_encoded\"] = train_df[\"label\"].map(label_map)\n",
        "\n",
        "# Verify encoding\n",
        "print(\"✅ Label encoding applied correctly!\")\n",
        "print(train_df[\"label_encoded\"].unique())  # Should now show `[0, 1, 2, 3]`\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vybiGNImnMwa",
        "outputId": "cbd628cb-b7de-472a-bf36-4347e52c8948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Label encoding applied correctly!\n",
            "[1 0 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📊 Columns in train_df:\", train_df.columns)\n",
        "print(\"📊 Columns in test_df:\", test_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpFZnu-58n8f",
        "outputId": "cfbe27c1-e648-4cf6-82f1-fca4cd32073f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Columns in train_df: Index(['claim', 'required_reannotation', 'label', 'justification',\n",
            "       'claim_date', 'speaker', 'original_claim_url', 'fact_checking_article',\n",
            "       'reporting_source', 'location_ISO_code', 'claim_types',\n",
            "       'fact_checking_strategies', 'questions', 'cached_original_claim_url',\n",
            "       'label_encoded'],\n",
            "      dtype='object')\n",
            "📊 Columns in test_df: Index(['claim', 'required_reannotation', 'label', 'justification',\n",
            "       'claim_date', 'speaker', 'original_claim_url', 'fact_checking_article',\n",
            "       'reporting_source', 'location_ISO_code', 'claim_types',\n",
            "       'fact_checking_strategies', 'questions', 'cached_original_claim_url',\n",
            "       'label_encoded'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label mapping\n",
        "label_map = {\n",
        "    \"Supported\": 0,\n",
        "    \"Refuted\": 1,\n",
        "    \"Not Enough Evidence\": 2,\n",
        "    \"Conflicting Evidence/Cherrypicking\": 3  # Ensure exact match\n",
        "}\n",
        "\n",
        "# Apply encoding safely\n",
        "train_df.loc[:, \"label_encoded\"] = train_df[\"label\"].map(label_map)\n",
        "test_df.loc[:, \"label_encoded\"] = test_df[\"label\"].map(label_map)\n",
        "\n",
        "# Verify encoding\n",
        "print(\"✅ Label encoding applied correctly!\")\n",
        "print(train_df[\"label_encoded\"].unique())  # Should show `[0, 1, 2, 3]`\n",
        "print(test_df[\"label_encoded\"].unique())  # Should also show `[0, 1, 2, 3]`\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTTZ9ibl84z6",
        "outputId": "2be67458-91cf-44dc-d098-cda51f05f739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Label encoding applied correctly!\n",
            "[1 0 2 3]\n",
            "[1 0 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: Feature Extraction using TF-IDF"
      ],
      "metadata": {
        "id": "TONU3xoldTnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_df[\"claim\"])\n",
        "X_test = vectorizer.transform(test_df[\"claim\"])\n",
        "y_train = train_df[\"label_encoded\"]\n",
        "y_test = test_df[\"label_encoded\"]\n"
      ],
      "metadata": {
        "id": "hkL3MGzAnxX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 6: Train Logistic Regression Model"
      ],
      "metadata": {
        "id": "wEW9xSLhdXTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clf = LogisticRegression(max_iter=500)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n"
      ],
      "metadata": {
        "id": "z0X77-imn0JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"🔍 Model Evaluation:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "LW-XXh7ue6MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- STEP 7: Evaluate Model Performance ----\n",
        "print(\"🔍 Model Evaluation:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2coGZI7kpiHk",
        "outputId": "fa6010be-071e-4777-a2e5-ac123ed3bb3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Model Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.48      0.64       122\n",
            "           1       0.71      1.00      0.83       305\n",
            "           2       1.00      0.29      0.44        35\n",
            "           3       0.00      0.00      0.00        38\n",
            "\n",
            "    accuracy                           0.75       500\n",
            "   macro avg       0.67      0.44      0.48       500\n",
            "weighted avg       0.74      0.75      0.69       500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ev2r for LR\n"
      ],
      "metadata": {
        "id": "cfelnG7efFVh"
      }
    },
    {
      "source": [
        "# ---- STEP 8: Convert Predictions to JSON for Ev2R Scoring ----\n",
        "reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "# Ensure 'retrieved_evidence' column exists before proceeding\n",
        "if 'retrieved_evidence' not in test_df.columns:\n",
        "    # If the column doesn't exist, create it and fill it (you might want to call your retrieve_best_evidence function here).\n",
        "    # Example: Fill with a default value\n",
        "    test_df['retrieved_evidence'] = 'No evidence found'  # Or call retrieve_best_evidence here to populate it\n",
        "    print(\"⚠️ 'retrieved_evidence' column was missing. It has been added with a default value.\")\n",
        "\n",
        "# Prepare JSON structure for evaluation\n",
        "predictions_json = [\n",
        "    {\"claim\": row[\"claim\"], \"evidence\": [{\"question\": \"Claim validation?\", \"answer\": row[\"retrieved_evidence\"], \"url\": \"https://example.com\"}], \"pred_label\": reverse_label_map[pred]}\n",
        "    for row, pred in zip(test_df.to_dict(orient=\"records\"), y_pred)\n",
        "]\n",
        "\n",
        "# Save predictions\n",
        "with open(\"dev_veracity_prediction.json\", \"w\") as f:\n",
        "    json.dump(predictions_json, f, indent=4)\n",
        "\n",
        "print(\"✅ Predictions saved as dev_veracity_prediction.json\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyc-9js3aAgH",
        "outputId": "56a86504-8e49-4e44-aa5c-50fd062a4912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ 'retrieved_evidence' column was missing. It has been added with a default value.\n",
            "✅ Predictions saved as dev_veracity_prediction.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/\"))  # List available files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX8wjPgPFRQQ",
        "outputId": "fbc52b92-03a1-4aec-a74a-dadae021e331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'dev_veracity_prediction.json', 'train.json', 'evaluate_veracity.py', 'dev.json', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load dev predictions (predicted labels & evidence)\n",
        "with open(\"dev_veracity_prediction.json\", \"r\") as f:\n",
        "    predictions = json.load(f)\n",
        "\n",
        "# Print sample predictions\n",
        "print(json.dumps(predictions[:10], indent=4))  # View first 5 predictions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV4CO6I-HiRU",
        "outputId": "07d2e28c-646e-4341-9640-11bb7eeb4599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"claim\": \"In a letter to Steve Jobs, Sean Connery refused to appear in an apple commercial.\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"Trump Administration claimed songwriter Billie Eilish Is Destroying Our Country In Leaked Documents\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"Due to Imran Khan's criticism of Macron's comments on Islam, French authorities cancelled the visas of 183 Pakistani citizens and deported 118 from the country.\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"UNESCO declared Nadar community as the most ancient race in the world.\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"Republican Matt Gaetz was part of a company that had to pay 75 million in hospice fraud. They stole from dying people.\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"The United States of America and its Western allies have been using their media outlets to publish articles based on fabricated information under allegations of non-compliance with the Chemical Weapons Convention.\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"More than 225,000 people dead, 225,000. The estimates are, if we\\u2019d have acted responsibly, there\\u2019d be 160,000 fewer dead than there are today, because of covid-19.\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"Why should you pay more taxes than Donald Trump pays? And that\\u2019s a fact. $750. Remember what he said when that was raised a while ago, how he only pays \\u2026 He said, \\u2018Because I\\u2019m smart. I know how to game the system.\\u2019\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"You\\u2019re watching the cheaters and all those people that send in the phony ballots. \\u2026 They want to have the count weeks after November 3.\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    },\n",
            "    {\n",
            "        \"claim\": \"You see the number today? 33.1 GDP. The biggest in the history of our country by almost triple, right? Almost triple. Now it\\u2019s very much bigger than any GDP we\\u2019ve ever had. You have to go back to the 1950s, and then it\\u2019s less than half. This is the greatest number, 33.1 percent.\",\n",
            "        \"evidence\": [\n",
            "            {\n",
            "                \"question\": \"Claim validation?\",\n",
            "                \"answer\": \"No evidence found\",\n",
            "                \"url\": \"https://example.com\"\n",
            "            }\n",
            "        ],\n",
            "        \"pred_label\": \"Refuted\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/evaluate_veracity.py\") #code given in the averitec website to compute this score"
      ],
      "metadata": {
        "id": "nA2iNFuMFo97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate_veracity import AVeriTeCEvaluator"
      ],
      "metadata": {
        "id": "6RurlWH92G_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCMUhWFKIK4l",
        "outputId": "a3243d4f-635c-44ed-ce9f-5e331abdd7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ev2r score for LR"
      ],
      "metadata": {
        "id": "291icinDekNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = AVeriTeCEvaluator()\n",
        "\n",
        "# Load ground truth data for comparison\n",
        "with open(\"dev.json\", \"r\") as f:\n",
        "    ground_truth_data = json.load(f)\n",
        "\n",
        "# Compute scores against the ground truth\n",
        "q_score = scorer.evaluate_questions_only(predictions, ground_truth_data)\n",
        "print(f\"📊 Question-Only Score: {q_score:.3f}\")\n",
        "\n",
        "qa_score = scorer.evaluate_questions_and_answers(predictions, ground_truth_data)\n",
        "print(f\"📊 Question-Answer Score: {qa_score:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc6a15C7CHhZ",
        "outputId": "8be9fbcc-686b-4545-815f-a76134e21c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Question-Only Score: 0.030\n",
            "📊 Question-Answer Score: 0.020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load existing predictions\n",
        "with open(\"dev_veracity_prediction.json\", \"r\") as f:\n",
        "    predictions = json.load(f)\n",
        "\n",
        "# Convert predictions to a dictionary for fast lookup\n",
        "predicted_claims = {pred[\"claim\"]: pred for pred in predictions}\n",
        "\n",
        "# Load ground truth labels\n",
        "with open(\"dev.json\", \"r\") as f:\n",
        "    references = json.load(f)\n",
        "\n",
        "# Check for missing claims and add default predictions\n",
        "missing_claims = [ref for ref in references if ref[\"claim\"] not in predicted_claims]\n",
        "\n",
        "for missing in missing_claims:\n",
        "    predictions.append({\n",
        "        \"claim\": missing[\"claim\"],\n",
        "        \"pred_label\": \"Not Enough Evidence\",  # Default label\n",
        "        \"evidence\": [{\"question\": \"N/A\", \"answer\": \"No evidence found\", \"url\": \"N/A\"}]\n",
        "    })\n",
        "\n",
        "# Save fixed predictions\n",
        "with open(\"dev_veracity_prediction_fixed.json\", \"w\") as f:\n",
        "    json.dump(predictions, f, indent=4)\n",
        "\n",
        "print(f\"✅ Added {len(missing_claims)} missing predictions. New file saved: dev_veracity_prediction_fixed.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v65G8xrxKhuq",
        "outputId": "0f97063d-0bd8-4f92-9610-d50e99ea4c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Added 0 missing predictions. New file saved: dev_veracity_prediction_fixed.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate_veracity import AVeriTeCEvaluator\n",
        "\n",
        "# Load the fixed predictions\n",
        "with open(\"dev_veracity_prediction_fixed.json\") as f:\n",
        "    predictions = json.load(f)\n",
        "\n",
        "# Load reference labels again\n",
        "with open(\"dev.json\") as f:\n",
        "    references = json.load(f)\n",
        "\n",
        "# Initialize evaluator\n",
        "scorer = AVeriTeCEvaluator()\n",
        "\n",
        "# Compute AVeriTeC Ev2R Score\n",
        "ev2r_score = scorer.evaluate_averitec_score(predictions, references)\n",
        "\n",
        "print(\"📊 AVeriTeC Ev2R Scores:\")\n",
        "for i, level in enumerate(scorer.averitec_reporting_levels):\n",
        "    print(f\" * Score @ {level}: {ev2r_score[i]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbyRGyzIKqxX",
        "outputId": "cc11f6ef-bf14-41b0-8db0-2c29c03d89c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 AVeriTeC Ev2R Scores:\n",
            " * Score @ 0.1: 0.006\n",
            " * Score @ 0.2: 0.000\n",
            " * Score @ 0.25: 0.000\n",
            " * Score @ 0.3: 0.000\n",
            " * Score @ 0.4: 0.000\n",
            " * Score @ 0.5: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df[[\"claim\", \"retrieved_evidence\"]].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYjX-DPfK_wS",
        "outputId": "da1a28c2-6489-497f-982b-b30ac03308c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               claim retrieved_evidence\n",
            "0  In a letter to Steve Jobs, Sean Connery refuse...  No evidence found\n",
            "1  Trump Administration claimed songwriter Billie...  No evidence found\n",
            "2  Due to Imran Khan's criticism of Macron's comm...  No evidence found\n",
            "3  UNESCO declared Nadar community as the most an...  No evidence found\n",
            "4  Republican Matt Gaetz was part of a company th...  No evidence found\n",
            "5  The United States of America and its Western a...  No evidence found\n",
            "6  More than 225,000 people dead, 225,000. The es...  No evidence found\n",
            "7  Why should you pay more taxes than Donald Trum...  No evidence found\n",
            "8  You’re watching the cheaters and all those peo...  No evidence found\n",
            "9  You see the number today? 33.1 GDP. The bigges...  No evidence found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "true_labels = [ref[\"label\"] for ref in references]\n",
        "pred_labels = [pred[\"pred_label\"] for pred in predictions]\n",
        "\n",
        "print(\"🔍 Classification Report:\")\n",
        "print(classification_report(true_labels, pred_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nobETGWLC8s",
        "outputId": "f59f5e4c-5e8d-461e-bff4-f6f4684c57f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Classification Report:\n",
            "                                    precision    recall  f1-score   support\n",
            "\n",
            "Conflicting Evidence/Cherrypicking       0.00      0.00      0.00        38\n",
            "               Not Enough Evidence       1.00      0.29      0.44        35\n",
            "                           Refuted       0.71      1.00      0.83       305\n",
            "                         Supported       0.98      0.48      0.64       122\n",
            "\n",
            "                          accuracy                           0.75       500\n",
            "                         macro avg       0.67      0.44      0.48       500\n",
            "                      weighted avg       0.74      0.75      0.69       500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "source": [
        "for pred, ref in zip(predictions[:5], references[:5]):\n",
        "    print(f\"🔍 CLAIM: {pred['claim']}\")\n",
        "    # Access the 'evidence' list and extract the answer from the first item\n",
        "    retrieved_evidence = pred['evidence'][0]['answer'] if pred['evidence'] else \"No evidence found\"\n",
        "    print(f\"🔹 Retrieved Evidence: {retrieved_evidence}\")\n",
        "    print(f\"✅ Reference Evidence: {ref['fact_checking_article']}\\n\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvEBlFZUL3uD",
        "outputId": "a54bdef8-9a32-435e-a60e-c6337729cf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 CLAIM: In a letter to Steve Jobs, Sean Connery refused to appear in an apple commercial.\n",
            "🔹 Retrieved Evidence: No evidence found\n",
            "✅ Reference Evidence: https://web.archive.org/web/20201130144023/https://checkyourfact.com/2020/11/03/fact-check-sean-connery-letter-steve-jobs-apple-1998/\n",
            "\n",
            "🔍 CLAIM: Trump Administration claimed songwriter Billie Eilish Is Destroying Our Country In Leaked Documents\n",
            "🔹 Retrieved Evidence: No evidence found\n",
            "✅ Reference Evidence: https://web.archive.org/web/20201103001419/https://leadstories.com/hoax-alert/2020/11/fact-check-trump-administration-did-not-claim-songwriter-billie-eilish-was-destroying-country-in-leaked-documents.html\n",
            "\n",
            "🔍 CLAIM: Due to Imran Khan's criticism of Macron's comments on Islam, French authorities cancelled the visas of 183 Pakistani citizens and deported 118 from the country.\n",
            "🔹 Retrieved Evidence: No evidence found\n",
            "✅ Reference Evidence: https://web.archive.org/web/20210629013122/https://www.indiatoday.in/fact-check/story/fact-check-fake-tweet-france-deporting-pakistanis-go-viral-imran-slams-macron-1737276-2020-11-02\n",
            "\n",
            "🔍 CLAIM: UNESCO declared Nadar community as the most ancient race in the world.\n",
            "🔹 Retrieved Evidence: No evidence found\n",
            "✅ Reference Evidence: https://web.archive.org/web/20210225110220/https://www.vishvasnews.com/english/viral/fact-check-unesco-did-not-declare-nadar-community-as-ancient-race-in-the-world-viral-claim-is-fake/\n",
            "\n",
            "🔍 CLAIM: Republican Matt Gaetz was part of a company that had to pay 75 million in hospice fraud. They stole from dying people.\n",
            "🔹 Retrieved Evidence: No evidence found\n",
            "✅ Reference Evidence: https://web.archive.org/web/20210713185816/https://www.factcheck.org/2019/10/misleading-posts-target-gop-rep-matt-gaetz/\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train xgb model"
      ],
      "metadata": {
        "id": "a_Xd4Mmre0lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Train & Test Data\n",
        "def load_data(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "train_df = load_data(\"train.json\")\n",
        "test_df = load_data(\"dev.json\")\n",
        "\n",
        "# Ensure column names are correct\n",
        "print(\"Train Columns:\", train_df.columns)\n",
        "print(\"Test Columns:\", test_df.columns)\n",
        "\n",
        "# Split train data for validation\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"label\"])\n"
      ],
      "metadata": {
        "id": "JUqD5tCQi5ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_df[\"claim\"])\n",
        "X_val = vectorizer.transform(val_df[\"claim\"])\n",
        "X_test = vectorizer.transform(test_df[\"claim\"])\n",
        "\n",
        "y_train = train_df[\"label_encoded\"]\n",
        "y_val = val_df[\"label_encoded\"]\n"
      ],
      "metadata": {
        "id": "eBF5_iMVysZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "xgb_model = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth=6, use_label_encoder=False, eval_metric=\"mlogloss\")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_val)\n",
        "print(\"📊 XGBoost Classification Report:\\n\", classification_report(y_val, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp-amCN2yjZU",
        "outputId": "7bf6c9cc-8369-4f6a-f3dd-525b9ec446bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [04:23:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 XGBoost Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.21      0.24       170\n",
            "           1       0.57      0.76      0.66       349\n",
            "           2       0.21      0.05      0.09        56\n",
            "           3       0.21      0.08      0.11        39\n",
            "\n",
            "    accuracy                           0.50       614\n",
            "   macro avg       0.32      0.27      0.27       614\n",
            "weighted avg       0.44      0.50      0.45       614\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ev2r scores for XGB"
      ],
      "metadata": {
        "id": "EE4pZbDffMRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# ... (your previous code to load data and prepare train_df) ...\n",
        "\n",
        "# Tokenize evidence for BM25\n",
        "train_df[\"tokenized_evidence\"] = train_df[\"questions\"].apply(lambda x: nltk.word_tokenize(str(x).lower()))\n",
        "\n",
        "# Create BM25 index\n",
        "bm25 = BM25Okapi(train_df[\"tokenized_evidence\"])\n",
        "\n",
        "def retrieve_best_evidence(claim):\n",
        "    tokenized_claim = nltk.word_tokenize(claim.lower())\n",
        "    scores = bm25.get_scores(tokenized_claim)\n",
        "    best_evidence_idx = scores.argmax()\n",
        "    return train_df.iloc[best_evidence_idx][\"questions\"] if scores.max() > 0 else \"No evidence found\"\n",
        "\n",
        "# Apply retrieval to test_df\n",
        "test_df[\"retrieved_evidence\"] = test_df[\"claim\"].apply(retrieve_best_evidence)\n"
      ],
      "metadata": {
        "id": "kbvSI3bv0csM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "predictions_json = [\n",
        "    {\n",
        "        \"claim\": row[\"claim\"],\n",
        "        \"evidence\": [\n",
        "            {\n",
        "                \"question\": \"Claim validation?\",\n",
        "                \"answer\": \" \".join([str(item) for item in row[\"retrieved_evidence\"]]) if isinstance(row[\"retrieved_evidence\"], list) else str(row[\"retrieved_evidence\"]), # Convert each item to string before joining\n",
        "                \"url\": row[\"fact_checking_article\"] if pd.notna(row[\"fact_checking_article\"]) else \"No URL available\"\n",
        "            }\n",
        "        ],\n",
        "        \"pred_label\": reverse_label_map[pred]\n",
        "    }\n",
        "    for row, pred in zip(test_df.to_dict(orient=\"records\"), xgb_model.predict(X_test))  # Using XGBoost predictions\n",
        "]\n",
        "\n",
        "with open(\"devv_veracity_prediction.json\", \"w\") as f:\n",
        "    json.dump(predictions_json, f, indent=4)\n",
        "\n",
        "print(\"✅ Predictions saved as devv_veracity_prediction.json\") # corrected the output file name"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz-2kB501Fxk",
        "outputId": "f830750f-8d5f-4b21-8693-4aaee7cf2798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Predictions saved as devv_veracity_prediction.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate_veracity import AVeriTeCEvaluator\n",
        "\n",
        "# Load reference labels (if available)\n",
        "with open(\"dev.json\") as f:\n",
        "    references = json.load(f)\n",
        "\n",
        "scorer = AVeriTeCEvaluator()\n",
        "averitec_score = scorer.evaluate_averitec_score(predictions_json, references)\n",
        "\n",
        "# Print scores\n",
        "for i, level in enumerate(scorer.averitec_reporting_levels):\n",
        "    print(f\"📊 Ev2R Score @ {level}: {averitec_score[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y820g1f_0p-9",
        "outputId": "91bda789-111d-4f4d-d38f-2ab4dd59193f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Ev2R Score @ 0.1: 0.746\n",
            "📊 Ev2R Score @ 0.2: 0.514\n",
            "📊 Ev2R Score @ 0.25: 0.402\n",
            "📊 Ev2R Score @ 0.3: 0.34\n",
            "📊 Ev2R Score @ 0.4: 0.19\n",
            "📊 Ev2R Score @ 0.5: 0.134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Random Forest Classifier model"
      ],
      "metadata": {
        "id": "hAQrqDS5fPeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_val)\n",
        "print(\"📊 Random Forest Classification Report:\\n\", classification_report(y_val, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH0_6BxJy3aA",
        "outputId": "7636ecad-06d2-495e-a457-351da7740bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Random Forest Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.02      0.03       170\n",
            "           1       0.57      1.00      0.73       349\n",
            "           2       0.00      0.00      0.00        56\n",
            "           3       0.00      0.00      0.00        39\n",
            "\n",
            "    accuracy                           0.57       614\n",
            "   macro avg       0.39      0.25      0.19       614\n",
            "weighted avg       0.60      0.57      0.42       614\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}